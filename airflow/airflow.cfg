[core]
# The folder where your airflow pipelines live, most likely a
# subfolder in a code repository
# This path must be absolute
dags_folder = /path/to/your/dags

# Hostname by providing a path to a callable that returns a hostname.
# The format is "package:function".
# For example: mycompany.myapp.get_hostname
hostname_callable = socket:getfqdn

# Default timezone in case supplied date times are naive
# can be utc (default), system, or any IANA timezone e.g. Europe/Amsterdam
default_timezone = utc

# Default DAG view. Valid values are: tree, graph, duration, gantt, landing_times
# Only used in webserver
dags_default_view = tree

# The executor class that airflow should use. Choices include
# SequentialExecutor, LocalExecutor, CeleryExecutor, DaskExecutor, KubernetesExecutor
executor = SequentialExecutor

# This defines the maximum number of active DAG runs per DAG. A value of -1 indicates there is no limit
max_active_runs_per_dag = 16

[database]
# The sqlAlchemy connection string to the metadata database.
# SqlAlchemy supports many different database engines.
# More information here: http://airflow.apache.org/docs/apache-airflow/stable/howto/set-up-database.html#database-uri
sql_alchemy_conn = sqlite:///C:\Users\ahmed\Downloads\GWEILPDW\Global-Wage-Equity-Inequality-Labor-Power-Data-Warehouse\airflow\airflow.db

[webserver]
# The base url of your website as airflow cannot guess what domain or
# cname you are using. This is used in automated emails that
# airflow sends to point links to the right web server
base_url = http://localhost:8080

# The ip specified when starting the web server
web_server_host = 0.0.0.0

# The port on which to run the web server
web_server_port = 8080

[logging]
# Airflow can store logs remotely in AWS S3, Google Cloud Storage or Elastic Search.
# Set this to True if you want to enable remote logging.
remote_logging = False

# Users must supply an Airflow connection id that provides access to the storage
# location.
remote_log_conn_id =